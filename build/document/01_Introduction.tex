%
\begin{isabellebody}%
\setisabellecontext{{\isadigit{0}}{\isadigit{1}}{\isacharunderscore}{\kern0pt}Introduction}%
%
\isadelimtheory
%
\endisadelimtheory
%
\isatagtheory
%
\endisatagtheory
{\isafoldtheory}%
%
\isadelimtheory
%
\endisadelimtheory
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupchapter{Snippets%
}
\isamarkuptrue%
%
\isamarkupsection{Earley%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Context-free grammars have been used extensively for describing the syntax of programming languages
and natural languages. Parsing algorithms for context-free grammars consequently play a large role in
the implementation of compilers and interpreters for programming languages and of programs which understand
or translate natural languages. Numerous parsing algorithms have been developed. Some are general,
in the sense that they can handle all context-free grammars, while others can handle only subclasses of
grammars. The latter, restricted algorithms tend to be much more efficient The algorithm described here
seems to be the most efficient of the general algorithms, and also it can handle a larger class of grammars
in linear time than most of the restricted algorithms.

A language is a set of strings over a finite set of symbols. We call these terminal symbols and represent
them by lowercase letters: a, b, c. We use a context-free grammar as a formal device for specifying which
strings are in the set. This grammar uses another set of symbols, the nonterminals, which we can think of
as syntactic classes. We use capitals for nonterminals: A, B, C. String of either terminals or non-terminals
are represented by greek letters: alpha, beta, gamma. The empty string is epsilon. There is a finite set of
productions or rewriting rules of the form A -> alpha. The nonterminal which stands for sentence is called the
root R of the grammar. The productions with a particular nonterminal A on their left sides are called the
alternatives of A. We write alpha => beta if exists gamma, delta, ny, A such taht a = gamma A delta and
beta = gamma ny delta and A -> ny is a production. We write alpha =>* beta if exists alpha0, alpha1, ...
alpham (m > =0) such that alpha = alpha0 => alpha1 => ... => alpham = beta The sequence alphai is called a
derivation of beta from alpha. A sentential form is a string alpha such the the root R =>* alpha. A sentence
is a sentential form consisting entirely of terminal symbols. The language defined by a grammar L(G) is the
set of its sentences. We may represent any sentential form in at least one way as a derivation tree or parse
tree reflecting the steps made in deriving it. The degree of ambiguity of a sentence is the number of its
distinct derivation trees. A sentence is unambiguous if it has degree 1 of ambiguity. A grammar is unambiguous
if each of its sentences is unambiguous. A grammar is reduced if every nonterminal appears in some derivation
of some sentence. A recognizer is an algorithm which takes a input a string and either accepts or rejects it
depending on whether or not the string is a sentence of the grammer. A parser is a recogizer which also outputs
the set of all legal derivation trees for the string.

The algorithm scans an input string X1, ..., Xn from left to right. As eachsymbol Xi is scanned, a set of
states Si is constructed which represents the condition of the recognition process at that point in the
scan. Each state in the set represents (1) a production such that we are currently scanning a portion of
the input string which is derived from its right side, (2) a point in that production which shows how much of the
production's right side we have recognized so far, (3) a pointer back to the position in the input string
at which we began to look for that instance of the production. In general, we operate on a state set Si as follows:
we process the states in the set in order, performing one of three operatins on each one depending on the form
of the state. These operations may add more states to Si and may also put states in a new state set Si+1. We
describe the operations by example: ... The predictor operation is applicable to a state when there is a nonterminal
to  the right of the dot. It causes us to add one new state to Si for each alternative of that nonterminal.
We put the dot at the beginning of the production in each new state, since we have not scanned any of its symbols yet.
The pointer is set to i, since the state was created in Si. Thus the predictor adds to Si all the productions
which might generate substrings beginning at Xi+1. The scanner is applicable in case there is a terminal to the right
of the dot. The scanner compares that symbol with Xi+1 and if they match, it adds the state to Si+1 with the dot
moved over one in the state to indicate that that terminal symbol has been scanned. If we finish processing Si and
Si+1 remains empty an error has occurred in the input string. Otherwise, we start to process Si+1.
The completer is applicable to a state if its dot is at the end of its production. It goes back to the state set
indicated by its pointer and adds all states from this state set which have the dot in front of its nonterminal.
It then moves over the dot. Intuitively, the origin state set is the state set we were in when we went looking
for that nonterminal. We have now found it, so we go back to all the states which caused us to look for it, and move
the dot over in these states to show that it has been successfully scanned. If the algorithm ever produces an Si+1
consisting of the single state S -> alpha dot, 0, n, then the sentence is part of the grammar. Note that the algorithm
is in effect a top-down parser in which we carry along all possible parses simultaneously in such a way that we can often
combine like subparses.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Scott%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
The Computer Science community has been able to automatically generate parsers for a very wide class of context
free languages. However, many parsers are still written manually, either using tool support or even completely
by hand. This is partly because in some application areas such as natural language processing and bioinformatics
we don not have the luxury of designing the language so that it is amendable to know parsing techniques, but also
it is clear that left to themselves computer language designers do not naturally write LR(1) grammars. A grammar
not only defines the syntax of a language, it is also the starting point for the definition of the semantics,
and the grammar which facilitates semantics definition is not usually the one which is LR(1). Given this difficulty
in constructing natural LR(1) grammars that support desired semantics, the general parsing techniques, such as
the CYK Younger \cite{Younger:1967}, Earley \cite{Earley:1970} and GLR Tomita \cite{Tomita:1985} algorithms, developed
for natural language processing are also of interest to the wider computer science community. When using grammars as
the starting point for semantics definition, we distinguish between recognizers which simply determine whether or not
a given string is in the language defined by a given grammar, and parserwhich also return some form of derivation
of the string, if one exists. In their basic form the CYK and Earley algorithms are recognizers while GLR-style
algorithms are designed with derivation tree construction, and hence parsing, in mind.

There is no known liner time parsing or recognition algorithm that can be used with all context free grammars.
In their recognizer forms the CYK algorithm is worst case cubic on grammars in Chomsky normal form and Earley's
algorithm is worst case cubic on general context free grammers and worst case n2 on non-ambibuous grammars.
General recognizers must, by definition, be applicable to ambiguous grammars. Tomita's GLR algorithm is of unbounded
polynomial order in the worst case. Expanding general recognizers to parser raises several problems, not least
because there can be exponentially many or even infinitely many derivations for a given input string. A cubic
recognizer which was modified to simply return all derivations could become an unbounded parser.
Of course, it can be argued that ambiguous grammars reflect ambiguous semantics and thus should not be used in
practice. This would be far too extreme a position to take. For example, it is well known that the if-else
statement in hthe AnSI-standard grammar for C is ambiguous, but a longest match resolution results in a linear
time parser that attach the else to the most recent if, as specified by the ANSI-C semantics. The ambiguous
ANSI-C grammar is certainly practical for parser implementation. However, in general ambiguity is not so easily handled,
and it is well known that grammar ambiguity is in fact undecidable Hopcroft \textit{et al} \cite{Hopcroft:2006}, thus
we cannot expect a parser generator simply to check for ambiguity inthe grammar and report the problem back to the user.
Another possiblity is to avoid the issue by just returning one derivation. However, if only one derivation is returned
then this creates problems for a user who wants all derivations and, even in the case where only one derivation is
required, there is the issue of ensuring that it is the required derivationthat is returned. A truely general parser
will reutrn all possible derivations in some form. Perhaps the most well known representation is the shared packed
parse foreset SPPF described and used by Tomita \cite{Tomita:1985}. Tomita's description of the representation does ont allow
for the infinitely many derivations which arise from grammars which contain cycles, the source adapt the SPPF representation
to allow these. Johnson \cite{Johnson:1991} has shown that Tomita-style SPPFs are worst case unbounded polynomial size. Thus
using such structures will alo turn any cubic recognition technique into a worst case unbounded polynomial parsing technique.
Leaving aside the potential increase in complexity when turning a recogniser into a parser, it is clear that this proccess is often difficult to carry
out correctly. Earley gave an algorithm for constructing derivations of a string accepted by his recognizer,
but this was subsequently shown by Tomita \cite{Tomita:1985} to return spurious derivations in certain cases.
Tomita's original version of his algorithm failed to terminate on grammars with hidden left recursio and, as remarked above
, had no mechanism for contructing complete SPPFs for grammers with cycles.

A shared packed parse forest SPPF is a representation designed to reduce the space required to represent multiple derivation
trees for an ambiguous sentence. In an SPPF, nodes which have the same tree below them are shared and nodes which correspond
to different derivations of the same substring from the same non-terminal are combined by creating a packed node for each
family of children. Nodes can be packed only if their yields correspond to the same portion of the input string. Thus, to make it easier
to determine whether two alternates can be packed under a given node, SPPF nodes are labelled with a triple (x,i,j) where
$a_{j+1} \dots a_i$ is a substring matched by x. To obtain a cubic algorithm we use binarised SPPFs which contain intermediate additional
nodes but which are of worst case cubic size. (EXAMPlE SPPF running example???)

We can turn earley's algorithm into a correct parser by adding pointers between items rather than instances of non-terminals, and labelling th epointers
in a way which allows a binariesd SPPF to be constructed by walking the resulting structure. However, inorder to
construct a binarised SPPF we also have to introduce additional nodes for grammar rules of length greater than two,
complicating the final algorithm.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Aycock%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Earley's parsing algorithm is a general algorithm, capable of parsing according to any context-free
grammar. General parsing algorithms like Earley parsing allow unfettered expression of ambiguous grammar
contructs which come up often in practice (REFERENCE).

Earley parsers operate by constructing a sequence of sets, sometime called Earley sets. Given an input
$x_1 x_2 \dots x_n$ the parser builds $n+1$ sets: an initial set $S_0$ and one set $S_i$ for each input
symbol $x_i$. Elements of these sets are referred to as Earley items, which consist of three parts:
a grammar rule, a position in the right-hand side of the rule indicating how much of that rule has been
seen and a pointer to an earlier Earley set. Typically Earley items are written as $\dots$ where the position
in the rule's right-hand side is denoted by a dot and $j$ is a pointer to set $S_j$.
An Earley set $S_i$ is computed from an initial set of Earley items in $S_i$ and $S_{i+1}$ is initialized, by
applying the followingn three steps to the items in $S_i$ until no more can be added. $\dots$
An item is added to a set only if it is not in the set already. The initial set $S_0$ contains the items $\dots$
to begin with. If the final set contains the item $\dots$ then the input is accepted.

We have not used a lookahead in this description of Earley parsing since it's primary purpose is to
increase the efficieny of the Earley parser on a large class of grammars (REFERENCE).

In terms of implementation, the Earley sets are built in increasing order as the input is read. Also,
each set is typically represented as a list of items. This list representation of a set is particularly
convenient, because the list of items acts as a work queue when building the sets: items are examined
in order, applying the transformations as necessary: items added to the set are appended onto the end of
the list.

At any given point $i$ in the parse, we have two partially constructed sets. Scanner may add items to
$S_{i+1}$ and $S_i$ may have items added to it by Predictor and Completer. It is this latter possibility,
adding items to $S_i$ while representing sets as lists, which causes grief with epsilon-rules.
When Completer processes an item A -> dot, j which corresponds to the epsilon-rule A -> epsiolon, it must
look through $S_j$ for items with the dot before an A. Unfortunately, for epsilon-rule items, j is always
equal to i. Completer is thus looking through the partially constructed set $S_i$. Since implementations
process items in $S_i$ in order, if an item B -> alpha dot A beta, k is added to $S_i$ after Completer
has processed A -> dot, j, Completer will never add B -> \alpha A dot \beta, k to $S_i$. In turn, items
resulting directly and indirectly from B -> \alpha A dot \beta, k will be omitted too. This effectively
prunes protential derivation paths which might cause correct input to be rejected. (EXAMPLE)
Aho \textit{et al} \cite{Aho:1972} propose the stay clam and keep running the Predictor and Completer
in turn until neither has anything more to add. Earley himself suggest to have the Completer note that
the dot needed to be moved over A, then looking for this whenever future items were added to $S_i$.
For efficiency's sake the collection of on-terminals to watch for should be stored in a data structure
which allows fast access. Neither approach is very satisfactory. A third solution \cite{Aycoack:2002}
is a simple modification of the Predictor based on the idea of nullability. A non-terminal A is said to be
nullable if A derives star epsilon. Terminal symbols of course can never be nullable. The nullability of
non-terminals in a grammar may be precomputed using well-known techniques \cite{Appel:2003} \cite{Fischer:2009}
Using this notion the Predictor can be stated as follows: if A -> \alpha dot B \beta, j is in $S_i$,
add B -> dot \gamma, i to $S_i$ for all rules B -> \gamma. If B is nullable, also add A -> \alpha B dot \beta, j
to $S_i$. Explanation why I decided against it. Involves every grammar can be rewritten to not contain epsilon
productions. In other words we eagerly move the dot over a nonterminal if that non-terminal can derive epsilon
and effectivley disappear. The source implements this precomputation by constructing a variant of 
a LR(0) deterministic finite automata (DFA). But for an earley parser we must keep track of which parent
pointers and LR(0) items belong together which leads to complex and inelegant implementations \cite{McLean:1996}.
The source resolves this problem by constructing split epsilon DFAs, but still need to adjust the classical
earley algorithm by adding not only predecessor links but also causal links, and to construct the split
epsilon DFAs not the original grammar but a slightly adjusted equivalent grammar is used that encodes
explicitly information that is crucial to reconstructing derivations, called a grammar in nihilist normal form (NNF)
which might increase the size of the grammar whereas the authors note empirical results that the increase
is quite modest (a factor of 2 at most).

Example:
S -> AAAA, A -> a, A -> E, E -> epsilon, input a
$S_0$ S -> dot AAAA,0, A -> dot a, 0, A -> dot E, 0, E -> dot, 0, A -> E dot, 0, S -> A dot AAA, 0
$S_1$ A -> a dot, 0, S -> A dot AAA, 0, S -> AA dot AA, 0, A -> dot a, 1, A -> dot E, 1, E -> dot, 1, A -> E dot, 1, S -> AAA dot A, 0%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Related Work%
}
\isamarkuptrue%
%
\isamarkupsubsection{Related Parsing Algorithms%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Tomita \cite{Tomita:1987} presents an generalized LR parsing algorithm for augmented
context-free grammars that can handle arbitrary context-free grammars.

Izmaylova \textit{et al} \cite{Izmaylova:2016} develop a general parser 
combinator library based on memoized Continuation-Passing Style (CPS) recognizers that supports all
context-free grammars and constructs a Shared Packed Parse Forest (SPPF) in worst case cubic time and space.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsection{Related Verification Work%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Obua \textit{et al} \cite{Obua:2017} introduce local lexing, a novel parsing concept which interleaves
lexing and parsing whilst allowing lexing to be dependent on the parsing process. They base their
development on Earley's algorithm and have verified the correctness with respect to its local lexing
semantics in the theorem prover Isabelle/HOL. The background theory of this Master's thesis is based
upon the local lexing entry \cite{LocalLexing-AFP} in the Archive of Formal Proofs.

Lasser \textit{et al} \cite{Lasser:2019} verify an LL(1) parser generator using the Coq proof assistant.

Barthwal \textit{et al} \cite{Barthwal:2009} formalize background theory
about context-free languages and grammars, and subsequently verify an SLR automaton and parser produced
by a parser generator.

Blaudeau \textit{et al} \cite{Blaudeau:2020} formalize the metatheory on Parsing expression grammars (PEGs) and
build a verified parser interpreter based on higher-order parsing combinators for expression grammars
using the PVS specification language and verification system. Koprowski \textit{et al} \cite{Koprowski:2011}
present TRX: a parser interpreter formally developed in Coq which also parses expression grammars.

Jourdan \textit{et al} \cite{Jourdan:2012} present a validator which checks if a context-free grammar
and an LR(1) parser agree, producing correctness guarantees required by verified compilers.

Lasser \textit{et al} \cite{Lasser:2021} present the verified parser CoStar based on the ALL(*) algorithm.
They proof soundness and completeness for all non-left-recursive grammars using the Coq proof assistant.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Future Work%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Different approaches:

(1) SPPF style parse trees as in Scott et al -> need Imperative/HOL for this

Performance improvements:

(1) Look-ahead of k or at least 1 like in the original Earley paper.
(2) Optimize the representation of the grammar instead of single list, group by production, ...
(3) Keep a set of already inserted items to not double check item insertion.
(4) Use a queue instead of a list for bins.
(5) Refine the algorithm to an imperative version using a single linked list and actual pointers instead
    of natural numbers.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Parse tree disambiguation:

Parser generators like YACC resolve ambiguities in context-free grammers by allowing the user
the specify precedence and associativity declarations restricting the set of allowed parses. But they
do not handle all grammatical restrictions, like 'dangling else' or interactions between binary operators
and functional 'if'-expressions.

Grammar rewriting:

Adams \textit{et al} \cite{Adams:2017} describe a grammar rewriting approach reinterpreting CFGs as
the tree automata, intersectiong them with tree automata encoding desired restrictions and reinterpreting
the results back into CFGs.

Afroozeh \textit{et al} \cite{Afroozeh:2013} present an approach to specifying operator precedence
based on declarative disambiguation rules basing their implementation on grammar rewriting.

Thorup \cite{Thorup:1996} develops two concrete algorithms for disambiguation of grammars based on the idea of 
excluding a certain set of forbidden sub-parse trees.

Parse tree filtering:

Klint \textit{et al} \cite{Klint:1997} propose a framework of filters to describe and compare a wide
range of disambiguation problems in a parser-independent way. A filter is a function that selects
from a set of parse trees the intended trees.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupchapter{Introduction%
}
\isamarkuptrue%
%
\isamarkupsection{Motivation%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
some introduction about parsing, formal development of correct algorithms: an example based on
earley's recogniser, the benefits of formal methods, LocalLexing and the Bachelor thesis.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
work with the snippets, reformulate!%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Structure%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
standard blabla%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Related Work%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
see folder and bibliography%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Contributions%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
what did I do, what is new%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimtheory
%
\endisadelimtheory
%
\isatagtheory
%
\endisatagtheory
{\isafoldtheory}%
%
\isadelimtheory
%
\endisadelimtheory
%
\end{isabellebody}%
\endinput
%:%file=01_Introduction.tex%:%
%:%24=8%:%
%:%28=10%:%
%:%40=13%:%
%:%41=14%:%
%:%42=15%:%
%:%43=16%:%
%:%44=17%:%
%:%45=18%:%
%:%46=19%:%
%:%47=20%:%
%:%48=21%:%
%:%49=22%:%
%:%50=23%:%
%:%51=24%:%
%:%52=25%:%
%:%53=26%:%
%:%54=27%:%
%:%55=28%:%
%:%56=29%:%
%:%57=30%:%
%:%58=31%:%
%:%59=32%:%
%:%60=33%:%
%:%61=34%:%
%:%62=35%:%
%:%63=36%:%
%:%64=37%:%
%:%65=38%:%
%:%66=39%:%
%:%67=40%:%
%:%68=41%:%
%:%69=42%:%
%:%70=43%:%
%:%71=44%:%
%:%72=45%:%
%:%73=46%:%
%:%74=47%:%
%:%75=48%:%
%:%76=49%:%
%:%77=50%:%
%:%78=51%:%
%:%79=52%:%
%:%80=53%:%
%:%81=54%:%
%:%82=55%:%
%:%83=56%:%
%:%84=57%:%
%:%85=58%:%
%:%86=59%:%
%:%87=60%:%
%:%88=61%:%
%:%89=62%:%
%:%90=63%:%
%:%91=64%:%
%:%92=65%:%
%:%101=68%:%
%:%113=71%:%
%:%114=72%:%
%:%115=73%:%
%:%116=74%:%
%:%117=75%:%
%:%118=76%:%
%:%119=77%:%
%:%120=78%:%
%:%121=79%:%
%:%122=80%:%
%:%123=81%:%
%:%124=82%:%
%:%125=83%:%
%:%126=84%:%
%:%127=85%:%
%:%128=86%:%
%:%129=87%:%
%:%130=88%:%
%:%131=89%:%
%:%132=90%:%
%:%133=91%:%
%:%134=92%:%
%:%135=93%:%
%:%136=94%:%
%:%137=95%:%
%:%138=96%:%
%:%139=97%:%
%:%140=98%:%
%:%141=99%:%
%:%142=100%:%
%:%143=101%:%
%:%144=102%:%
%:%145=103%:%
%:%146=104%:%
%:%147=105%:%
%:%148=106%:%
%:%149=107%:%
%:%150=108%:%
%:%151=109%:%
%:%152=110%:%
%:%153=111%:%
%:%154=112%:%
%:%155=113%:%
%:%156=114%:%
%:%157=115%:%
%:%158=116%:%
%:%159=117%:%
%:%160=118%:%
%:%161=119%:%
%:%162=120%:%
%:%163=121%:%
%:%164=122%:%
%:%165=123%:%
%:%166=124%:%
%:%167=125%:%
%:%176=128%:%
%:%188=131%:%
%:%189=132%:%
%:%190=133%:%
%:%191=134%:%
%:%192=135%:%
%:%193=136%:%
%:%194=137%:%
%:%195=138%:%
%:%196=139%:%
%:%197=140%:%
%:%198=141%:%
%:%199=142%:%
%:%200=143%:%
%:%201=144%:%
%:%202=145%:%
%:%203=146%:%
%:%204=147%:%
%:%205=148%:%
%:%206=149%:%
%:%207=150%:%
%:%208=151%:%
%:%209=152%:%
%:%210=153%:%
%:%211=154%:%
%:%212=155%:%
%:%213=156%:%
%:%214=157%:%
%:%215=158%:%
%:%216=159%:%
%:%217=160%:%
%:%218=161%:%
%:%219=162%:%
%:%220=163%:%
%:%221=164%:%
%:%222=165%:%
%:%223=166%:%
%:%224=167%:%
%:%225=168%:%
%:%226=169%:%
%:%227=170%:%
%:%228=171%:%
%:%229=172%:%
%:%230=173%:%
%:%231=174%:%
%:%232=175%:%
%:%233=176%:%
%:%234=177%:%
%:%235=178%:%
%:%236=179%:%
%:%237=180%:%
%:%238=181%:%
%:%239=182%:%
%:%240=183%:%
%:%241=184%:%
%:%242=185%:%
%:%243=186%:%
%:%244=187%:%
%:%245=188%:%
%:%246=189%:%
%:%247=190%:%
%:%256=193%:%
%:%260=195%:%
%:%272=197%:%
%:%273=198%:%
%:%274=199%:%
%:%275=200%:%
%:%276=201%:%
%:%277=202%:%
%:%286=205%:%
%:%298=208%:%
%:%299=209%:%
%:%300=210%:%
%:%301=211%:%
%:%302=212%:%
%:%303=213%:%
%:%304=214%:%
%:%305=215%:%
%:%306=216%:%
%:%307=217%:%
%:%308=218%:%
%:%309=219%:%
%:%310=220%:%
%:%311=221%:%
%:%312=222%:%
%:%313=223%:%
%:%314=224%:%
%:%315=225%:%
%:%316=226%:%
%:%317=227%:%
%:%318=228%:%
%:%319=229%:%
%:%328=234%:%
%:%340=237%:%
%:%341=238%:%
%:%342=239%:%
%:%343=240%:%
%:%344=241%:%
%:%345=242%:%
%:%346=243%:%
%:%347=244%:%
%:%348=245%:%
%:%349=246%:%
%:%350=247%:%
%:%351=248%:%
%:%355=252%:%
%:%356=253%:%
%:%357=254%:%
%:%358=255%:%
%:%359=256%:%
%:%360=257%:%
%:%361=258%:%
%:%362=259%:%
%:%363=260%:%
%:%364=261%:%
%:%365=262%:%
%:%366=263%:%
%:%367=264%:%
%:%368=265%:%
%:%369=266%:%
%:%370=267%:%
%:%371=268%:%
%:%372=269%:%
%:%373=270%:%
%:%374=271%:%
%:%375=272%:%
%:%376=273%:%
%:%377=274%:%
%:%378=275%:%
%:%387=278%:%
%:%391=280%:%
%:%403=282%:%
%:%404=283%:%
%:%408=285%:%
%:%417=287%:%
%:%429=289%:%
%:%438=291%:%
%:%450=293%:%
%:%459=295%:%
%:%471=297%:%
