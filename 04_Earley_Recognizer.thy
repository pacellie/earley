(*<*)
theory "04_Earley_Recognizer"
  imports
    "03_Fixpoint_Earley_Recognizer"
begin
(*>*)

chapter\<open>Earley Recognizer Implementation \label{chap:04}\<close>

section\<open>The Executable Algorithm \label{sec:alg}\<close>

text\<open>
In Chapter \ref{chapter:3} we proved correctness of a set-based implementation of Earley's simplified recognizer algorithm. In this chapter we implement
an executable version. But instead of re-proving soundness and completeness
for the executable algorithm, we follow the approach of Jones \cite{Jones:1972}. We refine our set-based
approach from Chapter \ref{chapter:3} to a \textit{functional} list-based implementation and prove subsumption in both
directions, or each item generated by the list-based approach is also generated by the set-based approach
which implies soundness of the executable algorithm, and vice versa which implies in turn completeness.
We extend the algorithm of Chapter \ref{chapter:3} in a second orthogonal way by already adding the necessary
information to construct parse trees. We only introduce and explain the needed data structures but refrain
from presenting any proofs in this chapter since constructing parse trees is the primary subject of Chapter \ref{chap:05}.

First we introduce a new data representation: instead of a set of Earley items we work with the data structure @{term bins}: a list of static length
(@{term "|\<omega>| + 1"}) containing in turn bins implemented as variable length lists of Earley \textit{entries}. An entry consists of an
Earley item and a new data type @{term pointer} representing conceptually an imperative pointer describing the origin of its accompanying item. Table \ref{tab:earley-items-pointers}
illustrates the bins for our running example. There are three possible reasons,
corresponding to the three basic operations, for the existence of an entry with Earley item $x$ in a specific bin $k$:

\begin{itemize}
  \item It was predicted. In that case we consider it created from thin air and do not need to
    track any additional information, thus the pointer is @{term Null}. For our example, bin
    $B_0$ contains the entry $S \rightarrow \, \bullet x, 0, 0; \bot$ consisting of the item $S \rightarrow \, \bullet x, 0, 0$ and a @{term Null} pointer denoted by $\bot$.
  \item It was scanned. Then there exists another Earley item $x'$ in the previous bin $k-1$
    from which this item was computed. Hence, we keep a predecessor pointer @{term "mbox0 (Pre pre)"}
    where @{term pre} is a natural number indicating the index of item $x'$ in bin $k-1$. Table \ref{tab:earley-items-pointers} contains
    the entry $S \rightarrow \, x \bullet, 2, 3; 1$ in bin $B_3$, the predecessor pointer is $1$ (we omit the @{term Pre} constructor for readability) since this item
    was created by the item $S \rightarrow \, x \bullet, 2, 2$ of the entry at index $1$ in $B_2$.
  \item It was completed. Note that an item might be completed in more than one way. In each case the
    item $x$ has a complete reduction item $y$ in the current bin and a predecessor item $x'$ in the
    origin bin of $y$. We track this information by at least one reduction pointer
    (@{term "PreRed (k', pre, red) reds"}) where $k'$, $pre$, and $red$ are respectively the origin index of
    the complete item $y$ or the bin of item $x'$, $pre$ is the index of $x'$ in bin $k'$, and $red$ is the index
    of $y$ in the current bin $k$. The list @{term reds} contains other valid reduction triples for this item.
    This is illustrated by the entry $S \rightarrow \, S + S \bullet, 0, 5; (4,1,0), (2,0,1)$ in bin $B_5$ of
    Table \ref{tab:earley-items-pointers}. We omit the @{term PreRed} and list constructors again for readability. This entry (without the second reduction triple) was first created due to the complete item $S \rightarrow \, x \bullet,4,5$
    at index $0$ in bin $B_5$ and the predecessor item $S \rightarrow \, S + \bullet S, 0, 4$ at index $1$ in bin $B_4$, but we can also create
    it by the complete item $S \rightarrow \, S + S \bullet,2,5$ at index $1$ in bin $B_5$ and the predecessor item
    $S \rightarrow \, S + \bullet S, 0, 2$ at index $0$ in bin $B_2$, or the two possible ways to derive
    the input $\omega = (x + x) + x$ and $\omega = x + (x + x)$.
\end{itemize}

Additionally, we define two useful abbreviations @{term items} and @{term pointers} that map a given
bin to the list of items respectively pointers it consists of.
\<close>

text\<open>
  \begin{table}[htpb]
    \caption[Earley items with pointers running example]{Earley items with pointers for the grammar @{term \<G>}: $S \rightarrow \, x$, $S \rightarrow \, S + S$}\label{tab:earley-items-pointers}
    \centering
    \begin{tabular}{| l | l | l | l |}
          & $B_0$                                       & $B_1$                                           & $B_2$                                     \\
      \midrule
        0 & $S \rightarrow \, \bullet x, 0, 0; \bot$     & $S \rightarrow \, x \bullet, 0, 1; 0$           & $S \rightarrow \, S + \bullet S, 0, 2; 1$   \\
        1 & $S \rightarrow \, \bullet S + S, 0, 0; \bot$ & $S \rightarrow \, S \bullet + S, 0, 1; (0,1,0)$ & $S \rightarrow \, \bullet x, 2, 2; \bot$       \\
        2 &                                           &                                                 & $S \rightarrow \, \bullet S + S, 2, 2; \bot$   \\

      \midrule

          & $B_3$                                               & $B_4$                                     & $B_5$                                                    \\
      \midrule
        0 & $S \rightarrow \, x \bullet, 2, 3; 1$           & $S \rightarrow \, S + \bullet S, 2, 4; 2$   & $S \rightarrow \, x \bullet, 4, 5; 2$                    \\
        1 & $S \rightarrow \, S + S \bullet, 0, 3; (2,0,0)$ & $S \rightarrow \, S + \bullet S, 0, 4; 3$   & $S \rightarrow \, S + S \bullet, 2, 5; (4,0,0)$          \\
        2 & $S \rightarrow \, S \bullet + S, 2, 3; (2,2,0)$ & $S \rightarrow \, \bullet x, 4, 4; \bot$       & $S \rightarrow \, S + S \bullet, 0, 5; (4,1,0), (2,0,1)$ \\
        3 & $S \rightarrow \, S \bullet + S, 0, 3; (0,1,1)$ & $S \rightarrow \, \bullet S + S, 4, 4; \bot$   & $S \rightarrow \, S \bullet + S, 4, 5; (4,3,0)$          \\
        4 &                                                 &                                             & $S \rightarrow \, S \bullet + S, 2, 5; (2,2,1)$          \\
        5 &                                                 &                                             & $S \rightarrow \, S \bullet + S, 0, 5; (0,1,2)$          \\
    \end{tabular}
  \end{table}
\<close>

datatype pointer =
  Null
  | Pre nat
  | PreRed "nat \<times> nat \<times> nat" "(nat \<times> nat \<times> nat) list"

datatype 'a entry =
  Entry (item : "'a item") (pointer : pointer)

type_synonym 'a bin = "'a entry list"

type_synonym 'a bins = "'a bin list"

definition items :: "'a bin \<Rightarrow> 'a item list" where
  "items b = map item b"

definition pointers :: "'a bin \<Rightarrow> pointer list" where
  "pointers b = map pointer b"

text\<open>
Next we implement list-based versions of the @{term Init}, @{term Scan}, @{term Predict}, and
@{term Complete} operations. Function @{term Init_list} creates a list of (@{term "|\<omega>| + 1"}) empty lists
or bins. Subsequently, it constructs an initial bin containing entries consisting of initial items for all the
production rules that have the start symbol on their left-hand sides, and finally overwrites the
$0$-th bin with this initial bin.
\<close>

definition Init_list :: "'a cfg \<Rightarrow> 'a sentential \<Rightarrow> 'a bins" where
  "Init_list \<G> \<omega> \<equiv> 
    let bs = replicate ( |\<omega>| + 1) ([]) in
    let rs = filter (\<lambda>r. rule_head r = \<SS> \<G>) (\<RR> \<G>) in
    let b0 = map (\<lambda>r. (Entry (init_item r 0) Null)) rs in
    bs[0 := b0]"

text\<open>
Functions @{term Scan_list}, @{term Predict_list}, and @{term Complete_list} are defined analogously
to the definitions of @{term Scan}, @{term Predict}, and @{term Complete} and we only highlight noteworthy
differences. The set-based implementations take accumulated as arguments the index $k$ of the current
bin, the grammar @{term \<G>}, the input @{term \<omega>}, and the current set of Earley items $I$.
The list-based definitions are more specific. The $k$-th bin is no longer only conceptional and we replace the argument $I$ in the following ways: function
@{term Scan_list} takes as arguments the currently considered item $x$, its next \textit{terminal} symbol $a$ (as plain value and
not wrapped in an option) and the index @{term pre} of $x$ in the current bin $k$, and sets the predecessor pointer accordingly. Function @{term Predict_list}
only needs access to the next non-terminal symbol $N$ of $x$, and returns only entries with @{term Null} pointers. The implementation of @{term Complete_list}
is slightly more involved. It takes as arguments again $x$ and the index @{term red} of $x$ in the current bin $k$ (since $x$ is a
complete reduction item this time), but also the complete bins @{term bs}, since
it needs to find all potential predecessor items as well as their indices in the origin bin of $x$ (see
@{term find_with_index}), and sets the reduction triples accordingly.
\<close>

definition Scan_list :: "nat \<Rightarrow> 'a sentential \<Rightarrow> 'a  \<Rightarrow> 'a item \<Rightarrow> nat \<Rightarrow> 'a entry list" where
  "Scan_list k \<omega> a x pre \<equiv>
    if \<omega>!k = a then
      let x' = inc_item x (k+1) in
      [Entry x' (Pre pre)]
    else []"

definition Predict_list :: "nat \<Rightarrow> 'a cfg \<Rightarrow> 'a \<Rightarrow> 'a entry list" where
  "Predict_list k \<G> N \<equiv>
    let rs = filter (\<lambda>r. rule_head r = N) (\<RR> \<G>) in
    map (\<lambda>r. (Entry (init_item r k) Null)) rs"

fun filter_with_index' :: "nat \<Rightarrow> ('a \<Rightarrow> bool) \<Rightarrow> 'a list \<Rightarrow> ('a \<times> nat) list" where
  "filter_with_index' _ _ [] = []"
| "filter_with_index' i P (x#xs) = (
    if P x then (x,i) # filter_with_index' (i+1) P xs
    else filter_with_index' (i+1) P xs)"

definition filter_with_index :: "('a \<Rightarrow> bool) \<Rightarrow> 'a list \<Rightarrow> ('a \<times> nat) list" where
  "filter_with_index P xs = filter_with_index' 0 P xs"

definition Complete_list :: "nat \<Rightarrow> 'a item \<Rightarrow> 'a bins \<Rightarrow> nat \<Rightarrow> 'a entry list" where
  "Complete_list k x bs red \<equiv>
    let orig = bs ! item_origin x in
    let is = filter_with_index (\<lambda>x'. next_symbol x' = Some (item_rule_head x)) (items orig) in
    map (\<lambda>(x', pre). (Entry (inc_item x' k) (PreRed (item_origin x, pre, red) []))) is"

text\<open>
In our data representation a bin is just a simple list but it implements a set. Hence we need to make sure
that updating a bin (@{term bin_upd}) or inserting an additional entry into a bin maintains its set properties. Additionally, since it is possible to generate multiple
reduction pointers for the same item, we have to take care to update the pointer information accordingly, in particular merge reduction triples,
if the item of the entry to be inserted matches the item of an already present entry. Function @{term bin_upds} inserts
multiple entries into a specific bin. Note that an alternative but equivalent implementation of @{term bin_upds} is @{term "fold bin_upd es b"}.
We primarily choose the explicit definition since it simplified some of the proofs, but overall the choice is
stylistic in nature. Finally, function @{term bins_upd} updates the $k$-th bin by inserting the given
list of entries using function @{term bin_upds}.
\<close>

fun bin_upd :: "'a entry \<Rightarrow> 'a bin \<Rightarrow> 'a bin" where
  "bin_upd e' [] = [e']"
| "bin_upd e' (e#es) = (
    case (e', e) of
      (Entry x (PreRed px xs), Entry y (PreRed py ys)) \<Rightarrow> 
        if x = y then Entry x (PreRed py (px#xs@ys)) # es
        else e # bin_upd e' es
      | _ \<Rightarrow> 
        if item e' = item e then e # es
        else e # bin_upd e' es)"

fun bin_upds :: "'a entry list \<Rightarrow> 'a bin \<Rightarrow> 'a bin" where
  "bin_upds [] b = b"
| "bin_upds (e#es) b = bin_upds es (bin_upd e b)"

definition bins_upd :: "'a bins \<Rightarrow> nat \<Rightarrow> 'a entry list \<Rightarrow> 'a bins" where
  "bins_upd bs k es = bs[k := bin_upds es (bs!k)]"

text\<open>
The central piece for the list-based implementation is the function @{term Earley_bin_list'}. A
function call of the form @{term "Earley_bin_list' k \<G> \<omega> bs i"} completes the $k$-th bin starting from index $i$.
For the current item $x$ under consideration it first computes the new entries depending on the next
symbol of $x$ which can either be some terminal, we scan, or non-terminal symbol, we predict, or @{term None}, we complete.
And then updates the bins @{term bs} appropriately using the function @{term bins_upd}.
We have to define the function as a @{term partial_function},
since it might never terminate if it keeps appending newly generated items to the $k$-th bin it currently operates
on. We prove termination and highlight the relevant Isabelle specific details in Section \ref{sec:04-wellformedness}.
The function @{term Earley_bin_list} then fully completes the $k$-th bin, or starts its computation at index $0$, and thus corresponds to the function @{term Earley_bin} of Chapter \ref{chapter:3}.
\<close>

partial_function (tailrec) Earley_bin_list' :: "nat \<Rightarrow> 'a cfg \<Rightarrow> 'a sentential \<Rightarrow> 'a bins \<Rightarrow> nat \<Rightarrow> 'a bins" where
  "Earley_bin_list' k \<G> \<omega> bs i = (
    if i \<ge> |items (bs!k)| then bs
    else
      let x = items (bs!k) ! i in
      let bs' =
        case next_symbol x of
          Some a \<Rightarrow>
            if is_terminal \<G> a then
              if k < |\<omega>| then bins_upd bs (k+1) (Scan_list k \<omega> a x i)
              else bs
            else bins_upd bs k (Predict_list k \<G> a)
        | None \<Rightarrow> bins_upd bs k (Complete_list k x bs i)
      in Earley_bin_list' k \<G> \<omega> bs' (i+1))"

definition Earley_bin_list :: "nat \<Rightarrow> 'a cfg \<Rightarrow> 'a sentential \<Rightarrow> 'a bins \<Rightarrow> 'a bins" where
  "Earley_bin_list k \<G> \<omega> bs = Earley_bin_list' k \<G> \<omega> bs 0"

text\<open>
Finally, functions @{term Earley_list} and @{term \<E>arley_list} are structurally identical to functions
@{term Earley} respectively @{term \<E>arley} of Chapter \ref{chapter:3}, differing only in the type of the used operations and the
return type: bins or lists instead of set of items.
\<close>

fun Earley_list :: "nat \<Rightarrow> 'a cfg \<Rightarrow> 'a sentential \<Rightarrow> 'a bins" where
  "Earley_list 0 \<G> \<omega> = Earley_bin_list 0 \<G> \<omega> (Init_list \<G> \<omega>)"
| "Earley_list (Suc n) \<G> \<omega> = Earley_bin_list (Suc n) \<G> \<omega> (Earley_list n \<G> \<omega>)"

definition \<E>arley_list :: "'a cfg \<Rightarrow> 'a sentential \<Rightarrow> 'a bins" where
  "\<E>arley_list \<G> \<omega> = Earley_list |\<omega>| \<G> \<omega>"

section \<open>A Word on Performance\<close>

text\<open>
Earley \cite{Earley:1970} implements his recognizer algorithm in the imperative programming paradigm and provides an informal
argument for the running time $\mathcal{O}(n^3)$ where @{term "n = |\<omega>|"}. Our implementation
is purely functional, and one might expect a quite significant decrease in performance. In this section we
provide an informal argument showing that, although we cannot quite achieve the time complexity of an imperative
implementation, we are 'only' one order of magnitude slower or the running time of our implementation is
$\mathcal{O}(n^4)$. Then we summarize Earley's imperative implementation approach and the additional steps
that are needed to achieve the desired running time. Additionally, we sketch a slightly different and more complicated functional
implementation that achieves a theoretical running time of $\mathcal{O}(n^3 \log{n})$, and highlight possible further
performance improvements. Finally, we discuss the choice for our particular implementation.

We state the running time of our implementation of the algorithm in terms of the length $n$ of the input @{term \<omega>},
and provide an informal argument that its running time is $\mathcal{O}(n^4)$.
Each bin $B_j$ ($0 \le j \le n$) contains only items of the form @{term "Item r b i j"}.
The number of possible production rules $r$, and possible bullet positions $b$ are both independent
of $n$ and can thus be considered (possible large) constants. The origin $i$ is bounded by $0 \le i \le j$ and thus
depends on $j$ which is in turn dependent by $n$. Overall the number of items in each bin $B_j$ is $\mathcal{O}(n)$.

We also have @{term Init_list} $\in \mathcal{O}(n)$ since the function @{term replicate} takes time linear in the length of @{term \<omega>},
and functions @{term filter} and @{term map} operate at most on the size of the grammar @{term \<G>} or constant time.
We also know @{term Scan_list} $\in \mathcal{O}(n)$. The dominating term is surprisingly (@{term "\<omega>!k"}), since $0 \le k \le n$, and it computes
at most one new entry. Function @{term Predict_list} takes time in the the size of the grammar @{term \<G>},
due to the @{term filter} and @{term map} functions, or constant time, and computes at most @{term "|\<G>|"} new items.
Function @{term Complete_list} again takes linear time, since finding the origin bin of the given item
$x$ takes linear time, and functions @{term items}, @{term filter_with_index}, and @{term map}
operate on the origin bin which is of at most linear size. Consequently, the function also computes at most
$\mathcal{O}(n)$ new items. 

Updating a bin (@{term bin_upd}) with a single entry takes at most linear time, inserting $e$ new entries
(@{term bin_upds}) thus takes time $e \cdot \mathcal{O}(n)$, and hence function @{term bins_upd}
also runs in time $e \cdot \mathcal{O}(n)$. The analysis of function @{term Earley_bin_list'} is slightly
more involved. It computes the contents of a bin $B_j$, or it calls itself recursively at most $n$ times, since the number of items in any bin is $\mathcal{O}(n)$.
The time for one function execution is dominated by the time it takes to update the bins with the newly
created items whose number in turn depends on the operation we applied but is bounded in the worst case by $n$ during the @{term Complete_list} operation.
All the other operations of the function body run in at most linear time. Overall we have for the body of @{term Earley_bin_list'}: $\mathcal{O}(n) + e \cdot \mathcal{O}(n) = \mathcal{O}(n^2)$.
And thus @{term Earley_bin_list'} $\in \mathcal{O}(n^3)$. The same bound holds trivially for @{term Earley_bin_list}.
Since functions @{term \<E>arley_list} or @{term Earley_list} call @{term Earley_bin_list} once for each bin $B_j$ and $0 \le j \le n$,
the overall running time is $\mathcal{O}(n^4)$.

One might be tempted to think that the decrease in performance compared to an imperative implementation
is due to the fact that we are representing bins as functional lists and appending to and indexing into
bins which takes linear time and not constant time. This is not the case. Earley implements the algorithm as
follows. On the top-level bins are no longer a list but an array. Each bin is a singly-linked list, and
pointers are no longer represented by the type @{term pointer} but by actual pointers between entries.
The worst case running time of the algorithm is still $\mathcal{O}(n^4)$. The algorithm still iterates over
$n$ bins, traverses in the worst case $\mathcal{O}(n)$ items in each bin and for each item, the worst case
operation, completion, still generates $\mathcal{O}(n)$ new items that all have to be inserted into the
current bin which takes linear time for \textit{each} new item. To achieve the running time of $\mathcal{O}(n^3)$
we need to find a way to add a new item into a bin in constant time. In an imperative setting one obvious
way is to not only keep a singly-linked list of items and pointers but additionally a map. The keys are the items of the
list and the map stores as value for a specific item a pointer to itself or its position in the list. Insertion of a new item
into a bin then works as follows: if the item is already present in the map, we follow the pointer to the item and
update the pointers of the item in the list accordingly depending on the kind of item. Otherwise we just append the
item and its corresponding pointers to the list and insert the item and a pointer to its position
in the linked list into the map.

Sadly, this approach does not work in a functional setting. Appending an item to a list takes linear and
not constant time. But even if we preprend the new item onto the list there is another problem. We cannot
simply store pointers in the map that we can chase in constant time to the location of the item
in the list, but still have to store the index of the corresponding item. And consequently updating the pointer information
takes again linear time due to the indexing. One possible solution is to change one's point of view.
In the imperative approach the list serves two purposes: it represents the bin and is at the same time
a worklist for the algorithm. The map only optimizes performance. We can obtain a $\mathcal{O}(n^3 \log{n})$ functional
implementation if we consider the list only a worklist and the map (or its keys) the bin. We also need to adapt the
pointer datatype. Instead of wrapping indices representing predecessor or reduction items in the list,
a pointer should contain the actual items. E.g. a pointer is either @{term Null}, or @{term "Pre x'"}, or @{term "PreRed (x', y) xys"}.
Overall the running time for inserting a new item into a bin consists of prepending the item onto the worklist, or constant time,
and inserting the item into the map which can be done in logarithmic time. Thus, the overall running
time of this approach is $\mathcal{O}(n^3 \log{n})$.

Since we are already talking about performance, we highlight some of the more common performance improvements.
We can predict faster if we organize the grammar in a more efficient manner. Currently, the @{term Predict}
operation needs to pass through the whole grammar to find the alternatives for a specific non-terminal. The
first performance improvement is to group the production rules by their left-hand side non-terminals.
We can also complete more efficiently. The @{term Complete} operation scans through the origin bin of
an complete item, searching for items where the next symbol matches the rule head of the production rule
of the complete item. We can optimize this search by keeping an additional map from 'next symbol' non-terminals to
their corresponding items for each bin. Finally, as mentioned earlier, we omit implementing a lookahead terminal.
Note that, although these performance improvements might speed up the algorithm quite considerably, none of
them improve the worst case running time.

We decided against implementing the map-based functional approach with a running time of $\mathcal{O}(n^3 \log{n})$
and 'settle' for the current approach with a running time of $\mathcal{O}(n^4)$ due to two reasons.
The map-based functional approach is more complicated and the improvement of the running time, although
significant, still does not reach the optimum. If we optimize our approach only to achieve better performance,
we would like to achieve optimal performance, at least asymptotically. The current approach, appending items
to the list and using natural numbers as pointers, maps more easily to the imperative approach. Our original
idea was to refine the algorithm once more to an imperative version. But this exceeded the scope of this thesis and is worthwhile future work.
\<close>

section \<open>Sets or Bins as Lists \label{sec:sets}\<close>

text\<open>
In this section we prove that the list representation of bins, in particular updating a bin or bins with the
functions @{term bin_upd}, @{term bin_upds}, and @{term bins_upd}, fulfills the required set semantics.
We define a function @{term bins} that accumulates all bins into one set of Earley items.
Note that a call of the form @{term "Earley_bin_list' k \<G> \<omega> bs i"} iterates through the entries of
the $k$-th bin or the current worklist in ascending order starting at index $i$. All items at indices
@{term "j \<ge> i"} are untouched and thus can be considered future work. We make two further definitions capturing
the set of items which are already 'done'. The term @{term "bin_upto b i"} represents the items of a bin $b$
up to but not including the $i$-th index. Similarly, function @{term bins_upto} computes the set of
items consisting of the $k$-th bin up to but not including the $i$-th index and the items of all previous bins. 
\<close>

definition bins :: "'a bins \<Rightarrow> 'a items" where
  "bins bs = \<Union> { set (items (bs!k)) | k. k < |bs| }"

definition bin_upto :: "'a bin \<Rightarrow> nat \<Rightarrow> 'a items" where
  "bin_upto b i = { items b ! j | j. j < i \<and> j < |items b| }"

definition bins_upto :: "'a bins \<Rightarrow> nat \<Rightarrow> nat \<Rightarrow> 'a items" where
  "bins_upto bs k i = \<Union> { set (items (bs!l)) | l. l < k } \<union> bin_upto (bs!k) i"

text\<open>
The next six lemmas then proof the set semantics of updating one bin with one item (@{term bin_upd}),
multiple items (@{term bin_upds}), or updating a particular bin with multiple items (@{term bins_upd}).
The proofs are straightforward and respectively by induction on the bin $b$ for an arbitrary item $e$,
by induction on the items @{term es} to be inserted for an arbitrary bin $b$, or by definition of
@{term bin_upds} and @{term bins} using previously proven lemmas in the appropriate proofs.
\<close>

lemma set_items_bin_upd:
  "set (items (bin_upd e b)) = set (items b) \<union> { item e }"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma distinct_bin_upd:
  assumes "distinct (items b)"
  shows "distinct (items (bin_upd e b))"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma set_items_bin_upds:
  "set (items (bin_upds es b)) = set (items b) \<union> set (items es)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma distinct_bin_upds:
  assumes "distinct (items b)"
  shows "distinct (items (bin_upds es b))"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma bins_bins_upd:
  assumes "k < |bs|"
  shows "bins (bins_upd bs k es) = bins bs \<union> set (items es)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma distinct_bins_upd:
  assumes "distinct (items (bs!k))"
  shows "distinct (items (bins_upd bs k es ! k))"
(*<*)
  sorry
(*>*)

text\<open>
In our formalization we prove further basic lemmas about functions @{term bin_upd}, @{term bin_upds},
and @{term bins_upd}. In particular how updating bins changes the length of a bin, interacts with
indexing into a bin or does not change the ordering of the items in a bin. Furthermore, we prove
similar lemmas about functions @{term bin_upto} and @{term bins_upto} and their interplay with bin(s)
updates. We omit them for brevity.
\<close>

section \<open>Well-formedness and Termination \label{sec:04-wellformedness}\<close>

text\<open>
We also need to refine the notion of well-formed items to well-formed \textit{bin} items. An item is a well-formed bin
item for the $k$-th bin if it is a well-formed item and its end index coincides with $k$. We call
a bin well-formed if it only contains well-formed bin items and its items are distinct, and lift this
notion of well-formedness to the toplevel list of bins.
\<close>

definition wf_bin_item :: "'a cfg \<Rightarrow> 'a sentential \<Rightarrow> nat \<Rightarrow> 'a item \<Rightarrow> bool" where
  "wf_bin_item \<G> \<omega> k x \<equiv> wf_item \<G> \<omega> x \<and> item_end x = k"

definition wf_bin_items :: "'a cfg \<Rightarrow> 'a sentential \<Rightarrow> nat \<Rightarrow> 'a item list \<Rightarrow> bool" where
  "wf_bin_items \<G> \<omega> k xs \<equiv> \<forall>x \<in> set xs. wf_bin_item \<G> \<omega> k x"

definition wf_bin :: "'a cfg \<Rightarrow> 'a sentential \<Rightarrow> nat \<Rightarrow> 'a bin \<Rightarrow> bool" where
  "wf_bin \<G> \<omega> k b \<equiv> distinct (items b) \<and> wf_bin_items \<G> \<omega> k (items b)"

definition wf_bins :: "'a cfg \<Rightarrow> 'a list \<Rightarrow> 'a bins \<Rightarrow> bool" where
  "wf_bins \<G> \<omega> bs \<equiv> \<forall>k < |bs|. wf_bin \<G> \<omega> k (bs!k)"

text\<open>
Next we prove that inserting well-formed bin items maintains the well-formedness of a bin or bins.
The proofs are structurally analogous to those of Section \ref{sec:sets}.
\<close>

lemma wf_bin_bin_upd:
  assumes "wf_bin \<G> \<omega> k b"
  assumes "wf_bin_item \<G> \<omega> k (item e)"
  shows "wf_bin \<G> \<omega> k (bin_upd e b)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma wf_bin_bin_upds:
  assumes "wf_bin \<G> \<omega> k b"
  assumes "\<forall>x \<in> set (items es). wf_bin_item \<G> \<omega> k x"
  assumes "distinct (items es)"
  shows "wf_bin \<G> \<omega> k (bin_upds es b)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma wf_bins_bins_upd:
  assumes "wf_bins \<G> \<omega> bs"
  assumes "\<forall>x \<in> set (items es). wf_bin_item \<G> \<omega> k x"
  assumes "distinct (items es)"
  shows "wf_bins \<G> \<omega> (bins_upd bs k es)"
(*<*)
  sorry
(*>*)

text\<open>
Next we would like to proof that function @{term Earley_bin_list'} also maintains the well-formedness of
the bins. But since it is a partial function we first need to take a short excursion into function definitions
in Isabelle.
Intuitively, a recursive function terminates if for every recursive call the size of its input strictly decreases.
And normally all functions defined in Isabelle must be total. There are different ways to define a recursive function depending on the complexity of its termination:
(1) with the \textit{fun} keyword. Isabelle then tries to find a measure of the input that proves
termination. If successful we obtain an induction schema corresponding to the function definition.
(2) via the \textit{function} keyword. We then need to define and prove a suitable measure by hand.
(3) if the function is a partial function we need to define it with the keyword @{term partial_function}.
For tail-recursive functions the definition is straightforward, otherwise we have to wrap the
return type in an option to signal possible non-termination. But contrary to total functions we do
\textit{not} obtain the usual induction schema. To prove anything useful about a partial function we
have to define the set of inputs and a corresponding measure for which the function terminates and
subsequently prove an appropriate induction schema by hand.

As previously explained, in Section \ref{sec:alg} we defined the function @{term Earley_bin_list'} as a partial function
since a call of the form @{term "Earley_bin_list' k \<G> \<omega> bs i"} might never terminate if the function
keeps appending arbitrary new items to the $k$-th bin it currently operates on. But we know that the
newly generated are not arbitrary but well-formed bin items. From lemma @{term finiteness} of Chapter \ref{chapter:3} we also
know that the set of well-formed items is finite. Since we made sure to only add each item once to
a bin, the function @{term Earley_bin_list'} will eventually run out of new items to insert into
the bin it currently operates on and terminate.

In Isabelle we define the set of well-formed earley inputs as a set of quadruples consisting of the
index $k$ of the current bin, the grammar @{term \<G>}, the input @{term \<omega>}, and the bins @{term bs}.
Note that we not only require the bins to be well-formed but also suitable bounds on $k$ and the length
of the bins to make sure that we are not indexing outside the input or the bins as well as a well-formed
grammar to ensure we only generate well-formed bin items. We then define a suitable measure for the
termination of @{term "Earley_bin_list' k \<G> \<omega> bs i"} which intuitively corresponds to the number of well-formed bin
items that are still possible to generate from index $i$ onwards. Finally we prove an induction schema
for the function by complete induction on the measure of the input. We omit showing the schema explicitly
since it is rather verbose. But intuitively it partitions the function into five cases: the base case where we have
run out of items to operate on; one case for completion and prediction each; and two cases for scanning
covering the normal and the special case where $k$ exceeds the length of the input.
\<close>

definition wf_earley_input :: "(nat \<times> 'a cfg \<times> 'a sentential \<times> 'a bins) set" where
  "wf_earley_input = { 
    (k, \<G>, \<omega>, bs) | k \<G> \<omega> bs.
      k \<le> |\<omega>| \<and> |bs| = |\<omega>| + 1 \<and>
      wf_\<G> \<G> \<and>
      wf_bins \<G> \<omega> bs
  }"

fun earley_measure :: "nat \<times> 'a cfg \<times> 'a sentential \<times> 'a bins \<Rightarrow> nat \<Rightarrow> nat" where
  "earley_measure (k, \<G>, \<omega>, bs) i = card { x | x. wf_bin_item \<G> \<omega> k x } - i"

text\<open>
Concluding this section, we prove that we maintain the well-formedness of the input for the function @{term Earley_bin_list'}.
The proof is by induction using our new induction schema, lemma @{term wf_bins_bins_upd} and - straightforward and thus omitted - auxiliary lemmas
stating that the scanning, predicting and completing only generates well-formed bin items. The proofs
for functions @{term Earley_bin_list}, @{term Earley_list}, and @{term \<E>arley_list} are respectively
by definition, by induction on $k$ using additionally the fact that the initial bins are well-formed,
and once more by definition, using previously proven lemmas appropriately.
\<close>

lemma wf_earley_input_Earley_bin_list':
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input" 
  shows "(k, \<G>, \<omega>, Earley_bin_list' k \<G> \<omega> bs i) \<in> wf_earley_input"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma wf_earley_input_Earley_bin_list:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input" 
  shows "(k, \<G>, \<omega>, Earley_bin_list k \<G> \<omega> bs) \<in> wf_earley_input"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma wf_earley_input_Earley_list:
  assumes "wf_\<G> \<G>"
  assumes "k \<le> |\<omega>|"
  shows "(k, \<G>, \<omega>, Earley_list k \<G> \<omega>) \<in> wf_earley_input"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma wf_earley_input_\<E>arley_list:
  assumes "wf_\<G> \<G>"
  assumes "k \<le> |\<omega>|"
  shows "(k, \<G>, \<omega>, \<E>arley_list \<G> \<omega>) \<in> wf_earley_input"
(*<*)
  sorry
(*>*)

section \<open>Soundness\<close>

text\<open>
Now we are ready to prove subsumption in both directions. Since functions @{term Earley_list} and
@{term \<E>arley_list} are structurally identical to @{term Earley} respectively @{term \<E>arley}, the
main task for the next two sections is to show that function @{term Earley_bin_list} or @{term Earley_bin_list'}
computes the same items as the function @{term Earley_bin} that computes in turn the fixpoint of @{term Earley_step}.
We start with the easy direction: every item generated by the list-based approach is also present in the set-based
approach which implies soundness of the list-based algorithm. This is the easier direction due to the fact that during execution of the body of
@{term "Earley_bin_list'"} we only consider a single item $x$ in bin $k$ at position $i$
and apply the appropriate operation. In contrast, one execution of function @{term Earley_step} applies the scan,
predict and complete operations for all previously computed items.
\<close>

lemma Init_list_eq_Init:
  shows "bins (Init_list \<G> \<omega>) = Init \<G>"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Scan_list_sub_Scan:
  assumes "wf_bins \<G> \<omega> bs"
  assumes "bins bs \<subseteq> I"
  assumes "k < |bs|"
  assumes "k < |\<omega>|"
  assumes "x \<in> set (items (bs!k))"
  assumes "next_symbol x = Some a"
  shows "set (items (Scan_list k \<omega> a x pre)) \<subseteq> Scan k \<omega> I"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Predict_list_sub_Predict:
  assumes "wf_bins \<G> \<omega> bs"
  assumes "bins bs \<subseteq> I"
  assumes "k < |bs|"
  assumes "x \<in> set (items (bs!k))"
  assumes "next_symbol x = Some N"
  shows "set (items (Predict_list k \<G> N)) \<subseteq> Predict k \<G> I"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Complete_list_sub_Complete:
  assumes "wf_bins \<G> \<omega> bs"
  assumes "bins bs \<subseteq> I"
  assumes "k < |bs|"
  assumes "x \<in> set (items (bs!k))"
  assumes "is_complete x"
  shows "set (items (Complete_list k x bs red)) \<subseteq> Complete k I"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_bin_list'_sub_Earley_bin:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "bins bs \<subseteq> I"
  shows "bins (Earley_bin_list' k \<G> \<omega> bs i) \<subseteq> Earley_bin k \<G> \<omega> I"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_bin_list_sub_Earley_bin:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "bins bs \<subseteq> I"
  shows "bins (Earley_bin_list k \<G> \<omega> bs) \<subseteq> Earley_bin k \<G> \<omega> I"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_list_sub_\<E>:
  assumes "wf_\<G> \<G>"
  assumes "k \<le> |\<omega>|"
  shows "bins (Earley_list k \<G> \<omega>) \<subseteq> Earley k \<G> \<omega>"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma \<E>arley_list_sub_\<E>arley:
  assumes "wf_\<G> \<G>" 
  shows "bins (\<E>arley_list \<G> \<omega>) \<subseteq> \<E>arley \<G> \<omega>"
(*<*)
  sorry
(*>*)

section \<open>Completeness\<close>

definition nonempty_derives :: "'a cfg \<Rightarrow> bool" where
  "nonempty_derives \<G> \<equiv> \<forall>N. N \<in> set (\<NN> \<G>) \<longrightarrow> \<not> (\<G> \<turnstile> [N] \<Rightarrow>\<^sup>* [])"

lemma impossible_complete_item: \<comment>\<open>Detailed\<close>
  assumes "wf_\<G> \<G>"
  assumes "nonempty_derives \<G>"
  assumes "wf_item \<G> \<omega> x"
  assumes "sound_item \<G> \<omega> x"
  assumes "is_complete x" 
  assumes "item_origin x = k"
  assumes "item_end x = k"
  shows False
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Complete_Un_eq_nonterminal: \<comment>\<open>Detailed\<close>
  assumes "wf_\<G> \<G>"
  assumes "wf_items \<G> \<omega> I"
  assumes "sound_items \<G> \<omega> I"
  assumes "nonempty_derives \<G>"
  assumes "wf_item \<G> \<omega> x"
  assumes "item_end x = k"
  assumes "next_symbol z = Some a"
  assumes "is_nonterminal \<G> a"
  shows "Complete k (I \<union> {x}) = Complete k I"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_step_sub_Earley_bin_list': \<comment>\<open>Detailed: START WITH THIS\<close>
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "sound_items \<G> \<omega> (bins bs)"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  assumes "Earley_step k \<G> \<omega> (bins_upto bs k i) \<subseteq> bins bs"
  shows "Earley_step k \<G> \<omega> (bins bs) \<subseteq> bins (Earley_bin_list' k \<G> \<omega> bs i)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_step_sub_Earley_bin_list:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "sound_items \<G> \<omega> (bins bs)"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  assumes "Earley_step k \<G> \<omega> (bins_upto bs k 0) \<subseteq> bins bs"
  shows "Earley_step k \<G> \<omega> (bins bs) \<subseteq> bins (Earley_bin_list k \<G> \<omega> bs)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_bin_list'_idem: \<comment>\<open>Detailed: SECOND IS THIS\<close>
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "sound_items \<G> \<omega> (bins bs)"
  assumes "nonempty_derives \<G>"
  assumes "i \<le> j"
  shows "bins (Earley_bin_list' k \<G> \<omega> (Earley_bin_list' k \<G> \<omega> bs i) j) = bins (Earley_bin_list' k \<G> \<omega> bs i)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_bin_list_idem:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "sound_items \<G> \<omega> (bins bs)"
  assumes "nonempty_derives \<G>"
  shows "bins (Earley_bin_list k \<G> \<omega> (Earley_bin_list k \<G> \<omega> bs)) = bins (Earley_bin_list k \<G> \<omega> bs)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma funpower_\<pi>_step_sub_\<pi>_it:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "sound_items \<G> \<omega> (bins bs)"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  assumes "Earley_step k \<G> \<omega> (bins_upto bs k 0) \<subseteq> bins bs"
  shows "funpower (Earley_step k \<G> \<omega>) n (bins bs) \<subseteq> bins (Earley_bin_list k \<G> \<omega> bs)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_bin_sub_Earley_bin_list:
  assumes "(k, \<G>, \<omega>, bs) \<in> wf_earley_input"
  assumes "sound_items \<G> \<omega> (bins bs)"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  assumes "Earley_step k \<G> \<omega> (bins_upto bs k 0) \<subseteq> bins bs"
  shows "Earley_bin k \<G> \<omega> (bins bs) \<subseteq> bins (Earley_bin_list k \<G> \<omega> bs)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma Earley_sub_Earley_list:
  assumes "wf_\<G> \<G>"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  assumes "k \<le> |\<omega>|"
  shows "Earley k \<G> \<omega> \<subseteq> bins (Earley_list k \<G> \<omega>)"
(*<*)
  sorry
(*>*)

text\<open>\<close>

lemma \<E>arley_sub_\<E>arley_list:
  assumes "wf_\<G> \<G>"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  shows "\<E>arley \<G> \<omega> \<subseteq> bins (\<E>arley_list \<G> \<omega>)"
(*<*)
  sorry
(*>*)

section \<open>Correctness\<close>

definition recognizing_list :: "'a bins \<Rightarrow> 'a cfg \<Rightarrow> 'a sentential \<Rightarrow> bool" where
  "recognizing_list I \<G> \<omega> \<equiv> \<exists>x \<in> set (items (I!|\<omega>| )). is_finished \<G> \<omega> x"

theorem recognizing_list_iff_recognizing:
  assumes "wf_\<G> \<G>"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  shows "recognizing_list (\<E>arley_list \<G> \<omega>) \<G> \<omega> \<longleftrightarrow> recognizing (\<E>arley \<G> \<omega>) \<G> \<omega>"
(*<*)
  sorry
(*>*)

text\<open>\<close>

corollary correctness_list:
  assumes "wf_\<G> \<G>"
  assumes "is_sentence \<G> \<omega>"
  assumes "nonempty_derives \<G>"
  shows "recognizing_list (\<E>arley_list \<G> \<omega>) \<G> \<omega> \<longleftrightarrow> \<G> \<turnstile> [\<SS> \<G>] \<Rightarrow>\<^sup>* \<omega>"
(*<*)
  sorry
(*>*)

text\<open>
SNIPPET:

It is this latter possibility, adding items to $S_i$ while representing sets as lists, which causes grief with epsilon-rules.
When Completer processes an item A -> dot, j which corresponds to the epsilon-rule A -> epsiolon, it must
look through $S_j$ for items with the dot before an A. Unfortunately, for epsilon-rule items, j is always
equal to i. Completer is thus looking through the partially constructed set $S_i$. Since implementations
process items in $S_i$ in order, if an item B -> alpha dot A beta, k is added to $S_i$ after Completer
has processed A -> dot, j, Completer will never add B -> \alpha A dot \beta, k to $S_i$. In turn, items
resulting directly and indirectly from B -> \alpha A dot \beta, k will be omitted too. This effectively
prunes protential derivation paths which might cause correct input to be rejected. (EXAMPLE)
Aho \textit{et al} \cite{Aho:1972} propose the stay clam and keep running the Predictor and Completer
in turn until neither has anything more to add. Earley himself suggest to have the Completer note that
the dot needed to be moved over A, then looking for this whenever future items were added to $S_i$.
For efficiency's sake the collection of on-terminals to watch for should be stored in a data structure
which allows fast access. Neither approach is very satisfactory. A third solution \cite{Aycock:2002}
is a simple modification of the Predictor based on the idea of nullability. A non-terminal A is said to be
nullable if A derives star epsilon. Terminal symbols of course can never be nullable. The nullability of
non-terminals in a grammar may be precomputed using well-known techniques \cite{Appel:2003} \cite{Fischer:2009}
Using this notion the Predictor can be stated as follows: if A -> \alpha dot B \beta, j is in $S_i$,
add B -> dot \gamma, i to $S_i$ for all rules B -> \gamma. If B is nullable, also add A -> \alpha B dot \beta, j
to $S_i$. Explanation why I decided against it. Involves every grammar can be rewritten to not contain epsilon
productions. In other words we eagerly move the dot over a nonterminal if that non-terminal can derive epsilon
and effectivley disappear. The source implements this precomputation by constructing a variant of 
a LR(0) deterministic finite automata (DFA). But for an earley parser we must keep track of which parent
pointers and LR(0) items belong together which leads to complex and inelegant implementations \cite{McLean:1996}.
The source resolves this problem by constructing split epsilon DFAs, but still need to adjust the classical
earley algorithm by adding not only predecessor links but also causal links, and to construct the split
epsilon DFAs not the original grammar but a slightly adjusted equivalent grammar is used that encodes
explicitly information that is crucial to reconstructing derivations, called a grammar in nihilist normal form (NNF)
which might increase the size of the grammar whereas the authors note empirical results that the increase
is quite modest (a factor of 2 at most).

Example:
S -> AAAA, A -> a, A -> E, E -> epsilon, input a
$S_0$ S -> dot AAAA,0, A -> dot a, 0, A -> dot E, 0, E -> dot, 0, A -> E dot, 0, S -> A dot AAA, 0
$S_1$ A -> a dot, 0, S -> A dot AAA, 0, S -> AA dot AA, 0, A -> dot a, 1, A -> dot E, 1, E -> dot, 1, A -> E dot, 1, S -> AAA dot A, 0
\<close>

(*<*)
end
(*>*)